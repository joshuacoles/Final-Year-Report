






Slimpletic Integrator
Sympletic integrator
q

[
Iterations on the slimpletic integrator and its applications to physically informed loss functions
J D Coles
 Department of Physics, University of Bath, Claverton Down, Bath BA2 7AY, UK

The Slimpletic Integrator by Tsang et al. provides fractional error bounds when integrating non-conservative systems. In this paper we present iterations on this method using modern computational techniques improving its computational performance by a factor of , with a small fixed cost, for long term integrations on the order of  and orders . We present applications of this work to physically informed loss functions for identifying physical systems from observational data and use these in the training of a physics informed neural network to do this. This network, when trained, predicts Lagrangians that are accurate to an RMS error of  in  and  in .
]

Introduction



Neural networks (NNs) and other machine learning (ML) models are becoming an increasingly important part of modern research and scientific application as they show wider application to data intensive problems and make progress where prior computational methods have proved otherwise intractable.

One method of training ML models involves the construction of a loss functions, or equivalently reward functions, which frames the training process as a high-dimensionality optimisation problem. In this setting the various parameters of the model are varied such as to minimise the aggregate loss of the models action over a large collection of inputs, compared to outputs. It is thus the loss function that grounds the model in a given problem. Their construction varies but they exhibit some general characteristics as will be discussed.



The  (SI) is a non-conservative extension of the the , a conservative variational integrator, which enables numerical integration of non-conservative systems that exhibits well defined bounds on the error in the energy and other conserved quantities of the system. These are based on the non-conservative action approach developed by Galley and Galley et al..

 are numerical integrators for Hamiltonian systems which have the property of preserving the canonical symplectic 2-form of the system. 
This makes them widely applicable to physical fields such as orbital dynamics and  molecular dynamics, among others

 as they will by this nature,  preserve, or near preserve, the constants of motion of a system over a large number of integration steps.


Currently the  is primarily implemented using computer algebra systems such as SymPy which express and work with  mathematical expressions symbolically and allow for the computation of derivatives and integrals. These are general systems and are effective at working with mathematical expressions but pose limitations when seeking to increase computational speed or scale up to larger systems. Automatic-differentiation techniques are a collection of computational methods for the determination of the derivatives of large classes of regular scientific code in an efficient and accurate manner.


In this paper we focus on taking the existing mathematical framework of the  and adapting it to use more advanced computational methods and making it amenable to  to enable a wide class of new applications. These range from scalability to larger systems, for example in molecular dynamics, to its use as the foundation of a number of physics based loss functions which we will compare for empirical and theoretical suitability, with the goal that these could be used in the creation of machine learning models which encode physical knowledge and insight.










Preliminaries

Lagrangian Mechanics

Lagrangian mechanics is a formalism of classical mechanics which places emphasis on the energies and systems of the system and that allows a flexibility in the exact variables used to parameterise the system. As it is traditionally phrased it works to solve the problem of determining the evolution of a physical system, parameterised by some generalised degrees of freedom , across the timespan  from some initially known configuration. These generalised degrees of freedom are represented as the vector, , and this initial configuration .

This determination is done by constructing a functional,



where  is known as the action of the system,  is the standard time derivative of the systems path, and  the Lagrangian, being traditionally made up of two components , the kinetic energy of the system in some configuration at some time, and , the potential energy of the system. These are combined as,



A physical solution is arrived on by then employing Hamilton's principle of least Action which provides that the physical path, , is the one that extremises the functional  as defined in Equation eq:trad-action, ie one where the variation  with respect to any variation of the path  is zero. This equation can be found by solving the Euler-Lagrange equations,



for each generalised degree of . To each we can also associate an conjugate momentum  and generalised force ,



such that Equation eq:euler-lagrange reads in-line with Newton's second law, .

The focus on symmetries within the formalism comes to the fore however when we consider Noether's Theorem which states that every continuous symmetry of the action  has a corresponding conserved quantity. A simple example of this can be seen for degrees of freedom which are said to be cyclic in that the coordinate value itself, , does not appear directly in the Lagrangian. It can be readily seen from Equation eq:euler-lagrange that if this is the case then,



which implies that the conjugate momentum  is a conserved quantity of the system.

Non-Conservative Actions

Lagrangian mechanics as discussed applies only to conservative systems. While there exist other extensions to non-conservative systems, notably Rayleigh dissipation functions for simple dissipative functions, we will focus on the method put forward by Galley et al..

Within this method we incorporate non-conservative behaviour by formally double the degrees of freedom of the system discussed prior into two virtual paths  and considering a new non-conservative Lagrangian  given by,



This form can be consider as the contribution of the traditional conservative Lagrangian moving forwards along the path traced by , and then backwards (and hence negative) along that traced by . This additional term  is labelled the non-conservative potential, representing a coupling between the two paths (if it could be broken down cleanly into a form of  or similar then these could be absorbed into  leaving ). In practical considerations it is often useful (as will become clear shortly) to alter our choice of variables to instead be  and . Within this formalism we can also define a corresponding conjugate momenta,



To solve this system we follow in the same form as before, aiming to extemise a new action defined now in terms of , however crucially we do so in what is known as the physical limit (P.L), where  or equivalently . This gives the non-conservative Euler Lagrange equation,



note that only the Euler-Lagrange equation differentiating with respect to  survives (or equivalently for ). A more complete explanation of this process, along with a derivation can be found in.

A corresponding form of Noether's theorem can be shown to hold for these non-conservative systems where the Noether currents evolve in time due to the effect of the non-conservative coupling potential .

The 

The  is an adaption of a symplectic integrators to non-conservative Lagrangian mechanics, symplectic integrators being those which preserve the canonical symplectic 2-form of the system  and thus preserve many desired properties in integration.

Thus to understand the  method we first consider the application of a symplectic integrators to a traditional conservative system(This explanation takes its path from an unpublished paper by Tsang et al.). As before consider a system with  degrees of freedom represented by some . We take that this system is governed by some Lagrangian, .

With this setup we now choose to apply two successive procedures to our extremal path , first piecewise breakdown and then discretisation of the action itself. It is this ordering that differentiates this symplectic method from more traditional integrators such as Runge-Kutta (as can be seen in fig:integrator-compare) which solve similar systems by discretising the final equations of motion rather than the action.


      TODO


To start we break the trajectory down piecewise into a collection of  sub-paths  where , each defined on some portion of the whole timespan  such that they cover it completely with overlaps only at the boundaries of the intervals. These together are such that the there is the correspondence,



for all . These paths define a collection of points  which are their mutual values at the piecewise break points and the initial and final values for  and  respectively.

As each piecewise curve constitutes in and of itself a physically attained trajectory we can state that any path  which extremises the action for the whole timespan, must also extremise each piecewise portion, and visa-versa. 

This does not bring us closer to a numerical solution directly but allows us to freely split our integral domain as needed without loss of accuracy, and thus limit the error of our numerical-integration method which will necessarily increase with the number of time-steps or their size.

We now turn to the discretisation method itself. This is done using the Galerkin-Gauss-Lobatto (GGL) quadrature method of order  which approximates the integral from  to  using the intermediary points





where  and  are the ordered roots of the the derivative of the th Legendre polynomial  and . For a given choice of  this provides slimpletic maps that are accurate up an order . In turn we also define the interior points  to each piecewise trajectory.

These allow us to approximate the path of the system within this quadrature using the associated cardinal functions for the GGL quadrature, labelled , as . These provide a suitable approximation for the path derivative  by the use the derivative matrix,



which provides that,



In turn this allows to express the integral as,



where the label the approximate expression  and where  are the quadrature weights given by



Equation eq:discr-action-1 strung together across the different piecewise sub-trajectories defined in equation eq:pw-traj gives us the total discretised action of the system, shown in Equation eq:discr-action-2, that sets sympletic integrators apart from their more general counterparts. 



From here we then extremise this approximate path with respect to the mutual and interior points to obtain the equations of motion of the system as,


	L_d^n-1q_n + L_d^nq_n = 0 

	 L_d^nq^(i)_n = 0

where  is This first equation can be simplified further into two equivalent definitions for the discrete momentum ,



and a continuity constraint at these mutual overlap points  in the same manner as we required for  itself. Together Equations eq:Ld-interior-eom, eq:pi-n can be solved to determine the values of , from which Equation eq:pi-n+1 provides a form for determining .

This form is provided in terms of the traditional conservative Lagrangian  however can be readily adapted to the non-conservative Lagrangian formalism discussed in sec:intro-nc-actions by considering instead the non-conservative Euler-Lagrange Equation eq:nc-el, in effect substituting  for the non-conservative Lagrangian  and  for .

Continuing the consideration of Noether currents from prior sections, it can be shown that the continuous symmetries of the conservative action evolve as due to this now discretised . It should be noted that GGL discretisation does not preserve time-shift symmetry, and hence energy is non conserved under the operation, however we will show, in-line with previous work, that the fractional error is generally bounded over the integration.

Physics Informed Neural Networks

Physics informed neural networks (PINNs) are neural networks which include physical knowledge in their training processes. Their most common application is to solving PDEs where we take physically derived knowledge of the system as a prior in training and fit a neural network to solve some PDE incorporating the residuals into the models loss (loss functions will be discussed in more depth in sec:intro-lf).

Neural networks more generally are a collection of linear and non-linear components composed together to form complex non-linear functions. At their base level the linear components can be expressed as the simple linear equation  where  is a collection of weights for this component and  the biases; and the non-linear components are functions such as sigmoid or  which are included to stop collapse to linearity for the whole composition. These weights, biases, and other variables within the model constitute the model's parameters.

This composition is useful in that it can be shown that, with sufficient complexity, these forms are dense in the space of Borel measurable functions and hence can take the place of almost any physical function or mathematical procedure.

Loss functions

Loss functions are the primary leaning method of neural networks and in PINNs act as the place where physics steps in to connect our computational model to reality. Loss functions work by attempting to minimise an arbitrary loss value computed from the parameters and a large dataset of known inputs and outputs for the model.

A loss function traditionally takes the form,  where  is some chosen function and  and  are both real vectors in the output space of the model, representing the known models output and the known true output respectively.

Loss functions in PINNs are often comprised of two components. First there is the prediction or physical loss which may for example be made up of the residuals of the model under PDE, boundary, and initial conditions of the system. Secondly there a regularisation loss term which helps to penalise overfitting to the training data, expressed as complexity, within the model. This might be implemented as an   of a vector containing all parameters of the model.

Optimising this loss function can be thought of as a traditional minimisation problem, with common choices of the method being Stochastic gradient descent and Adam. These impose various requirements on the loss function, for example generally that it is differentiable(Although methods exist for non-differentiable functions.) and benefits from other properties such as higher-order differentiability, Lipschitz continuity, and convexity. The rate at which these methods apply gradient based updates is determined a provided learning rate, which will often be varied through training to quickly move towards minima during the initial stages, then tuned down to avoid jumping over the minima as we close in on the optimal solution.

Lagrangian Embeddings

Dealing computationally with non-conservative Lagrangians , for example as an output of a PINN, requires defining some mapping from a collection of real 	numbers, to a form of . This can be thought of as a function,



where  is a chosen embedding function and  is a subset of the overall function space of possible  values. The choice of embedding necessarily restricts and dictates the systems which the system can deal with thus its choice is crucial in the design of any system. Discussions of different embeddings used in this paper can be found in sec:eg-sys. An embedding can either be chosen to represent a system specifically by its physical parameters, or by some arbitrary function approximation scheme such as Fourier series or Taylor expansions.

It should be noted that these embeddings often will not have a unique correspondence with physical Lagrangians, ie the map is not injective up to physical behaviour. This has implications for optimisation in creating multiple minima as will be discussed.

Automatic-Differentiation, XLA, and Google's JAX

Automatic-differentiation is the process of computing the differential of regular code, ie. code that is substantially similar to code that one would normally write, with respect to one or more of its arguments. Google's JAX library contains one implementation of this in Python which works by passing tracers into the Python code which record the actions done to them such that a differential can be calculated.

JAX also incorporates the XLA (Accelerated Linear Algebra) library where it uses similar tracing methods to translate a restricted, but large, subset of Python code into a series of low-level eifficent operations which can be executed at speed on the GPU or CPU.

These two technologies together show great promise in the fields of machine learning and simulation techniques as they allow for code that would previously have to be written in low-level languages such as C or C++ to be expressed in Python with significantly reduced overhead at runtime, and easier use by researchers.

Method






Damped Forced Harmonic Oscillators

The main system used in the testing and evaluation of the system is the Damped Forced Harmonic Oscillators. It is one of the simplest non-conservative systems which we can model having a non-conservative Lagrangian given by,





for a system with mass , spring constant , damping constant  and time dependent force . This corresponds with the standard equation of motion,







Improvements to the  and their Physical Applications

Initially the existing slimpletic codebase, the , was rewritten using the JAX framework, the . We chose two key metrics, run-time duration and accuracy, to judge our model against the  and Runge-Kutta to various orders.

First to ensure that we maintained the error bounds expected for our model (as per sec:intro-si) we compared the fractional energy and momentum error for Runge-Kutta, the , and the  across the time evolution of a physical system.

Next we create two test-cases to measure performance of the integrator and its consequences on experimental capability. These were,


	Modelling systems for different timespans varying the number of iterations performed by the integrator.
	Modelling the same system across different orders of integration  to determine how the order of our method effects the performance.


These are designed to test the scaling behaviours of our model to provide insight into its applicability to larger systems, the third acting as a more realistic test case of its use in actual experimental physics. 

TOOD: There will be an additional test of modelling a lattice of DHOs modelling a physical system here but this came to me too late.

Applications to Loss Functions and Optimisation

Taking this  we then explore its different uses as a loss function for physical systems, in combination with different embedding functions.

While these are distinct components of a learning system, they are heavily linked as ideally we would define the physical components of our loss function primarily in terms of the trajectories, rather than the values of the embedding vector itself. Hence the embedding function and slimpletic integrator must be chained together before the physical loss component to provide it with the trajectories. We consider a number of choices for these two functions comparing their behaviour near known minima looking for convexity and smoothness.




Loss Function in the Training of PINNs

Finally we apply various loss function and embedding choices to a stand-in neural network observing how the loss varies across training, and the correctness of the outputs.

This model was based on the long short-term memory layer which is frequently used in time-series analysis and thus chosen as a suitable basis of our model. This involves generating large datasets of different physical systems within the domain of the chosen embedding as will be discussed.

Results and Specific Discussion

Improvements to the Slimpletic Integrator and their Physical Applications


    A comparison of the error in the energy, top, and momenta, bottom, as a fraction of the known analytic solution of the system, of the  simulating a damped harmonic oscillator. One can clearly observe that the fractional energy error is bounded and displays the oscillating behaviour that we expect from previous work, whereas the momenta error does not display such behaviour. This plot is based off its equivalent in the original paper.

As discussed we first verify that the required errors orders are preserved by the  when compared to the  and traditional non-variational integrators. This can be seen in fig:dho_energy_bounds where the fractional error of the system from the true analytic solution is compared directly against the performance of the  and Runge-Kutta order 2 and 4. We clearly observe that the fractional energy error remains bounded across the timeframe of iteration. This is not the case for the fractional momenta error however as fixed-time-step variational methods such as those implemented cannot be both slimpletic-momentum and momentum-energy preserving.


      A comparison of running a damped harmonic oscillator system for various numbers of iterations, with . The  runtime is split into two components where JIT time represents the one time fixed cost of compiling the function (see sec:intro-autodiff) and Computation time represents the actual time spent on computation.
  Each value is a mean of 4 runs, with the  being cut off early due after  minutes runtime for the next sample.


      A comparison of running a damped harmonic oscillator system for various values of , the order of the GGL quadrature, with . Each result being a mean of 4 independent runs.
	For both implementations we split the overall time into setup and computation as changing the method order requires re-discretising the Lagrangian and thus non-trivial work under both implementations.

Next we explored the time complexity of the system in terms of the iteration count, , and GGL quadrature order, . These are important to physical applications as they determine the limits of iteration accuracy when iterating over large timescales, error scaling as  and total timeframe being .

Focusing first on the time complexity in  as shown in fig:dho-n-runtime, we note that that the computation time of the  is much lower than that of the , with the  growing as  where . In addition we note that the fixed cost JIT compilation remains roughly constant until growing linearly from about  iterations.

This pattern is seen also in fig:dho-r-runtime when investigating the time-complexity in the order of the method. Again the JIT time remains roughly constant across the domain tested, and though it starts out initially higher than than the 's setup costs, the 's computation time quickly swamps this fixed cost.
Similar to the behaviour in , the computation time for the  is also linear  , remaining insignificant in the overall runtime. This is in comparison with the  where the setup time grows as  with  and the computation time as  with .

TODO: How do we report sig. fig again?

It should be noted that while increasing the order of the method will increase the precision, at higher orders we start encountering issues with the fixed precision float64 type used for calculation in the  compared the the arbitrary precision numerics employed in the .
Still however this represents an overall improvement in accuracy in physically meaningful simulations as the required precision could be more readily attained by decreasing  rather than increasing , avoiding the blowup of runtime observed in the , or the degradation of precision at high  in the . 

TODO: This is where the lattice modelling will go.



















Loss Functions and Optimisation

Moving now onto the loss function and choice of Lagrangian embedding function. We explored a number of loss functions through a combination of gradient-descent and visual inspection. A wide number of loss functions were tested however focusing on those surrounding systems of harmonic oscillators, we settled on DHO system specific embedding shown in eq:dho-sys-embedding, labelled DHO 3, and an adaption where an additional embedding parameter  was introduced as pre-factor on the entire non-conservative Lagrangian , labelled DHO 4. These were paired with a number of simple loss functions, table:loss-fns, drawing from physical knowledge and existing convention for neural network loss functions.

First we perform a visual inspection of the the Simple RMS loss function at various scales in DHO 3 and 4 as seen in fig:loss-function-behaviour. From this we observe mild non-convexity which is of concern

To test how this mild non-convexity translates into optimisation performance we subject a number of combinations to empirical optimisation tests where we attempt to attain a known true embedding from a number of random initial embedding values. table:optimisation-results (see caption for details) shows that that the addition of the global pre-factor  substantially increases the probability of successful convergence.  In addition we see that both the  and  values are important in the fitting process from the complete lack of convergence when only  is included, and that their relative weight seems to be of little importance at this stage.

This is promising as, while optimisation directly in the embedding space is a strictly simpler problem than with respect to the parameters of an associated neural network, it suggests that this mild non-convexity is an obstacle we can overcome.

*[t]
      The local behaviour of an equally weighted  and  RMS loss function, when viewed as a function of variation in each of the embedding variables independently, with others kept fixed. We observe roughly correct behaviour in all variables bar mass  and subsequently  for the DHO 4 embedding where  acts as a pre-factor for the whole non-conservative Lagrangian . This behaviour is to be expected as system without mass is non-physical and explodes to infinity.


A list of loss functions. Key: RMS = Root Mean Squared, a standard loss function in machine learning




Results of optimising the chosen loss function from table:loss-fns for  initially random embeddings (distributed uniformly in ) in the input space of the chosen embedding function, where convergence is defined as being within  of the true embedding at the end of a maximum of  iterations of the optimiser. Note that for DHO 4 an additional normalisation step of dividing by  was applied to remove the redundancy introduced by the pre-factor in when determining if convergence had been achieved. The true embedding that was optimised towards, in DHO 3 form, was  and systems were compared with .



PINNs and Approximating Damped Harmonic Oscillators

Finally we applied these techniques to an PINN. For our initial explorations in this space we focused on fitting to systems of damped harmonic oscillators as has been the through line of the work thus far. We chose the 3 parameter DHO embedding over the 4 parameter embedding with a pre-factor as while it was more effective when subject to direct gradient descent, we were concerned that the additional redundant embedding parameter would make learning the systems more complex and error prone. 

The dataset used () in training was a mixed selection, primarily it was comprised of uniformly distributed embeddings within the subset of the physical region of the embedding space (positive embedding values). In addition a fraction, approximately  for each, of the spring and damping constants were set to zero to ensure that the model was exposed to SHM and kinetic only systems.

For PINN training, accounting for the increased complexity of the optimisation problem now being with respect to the parameters of the model, we made two alterations to our loss functions. First we added a strong weight against non-physical negative embedding values in our loss function, taking the form of,



where  is a tolerance to allow for zero to be a non-penalised output and  is the strength of the penalisation. This was done as the non-physical behaviour of negative embedding values is less visible when considering the larger whole model optimisation problem.
In addition we also experimented with capping the values of the loss at large values to mitigate overflows when dealing with particularly bad fits, such as the initially random initialised weights before any training had commenced.

Model training was done in batches and was prone to explosion possible due to the non-convexity in some regions as discussed in sec:res-lf. Overall it was found thatÂ training could be made more stable by increasing batch sizes to  and manually tweaking loss weightings (for example between the  and  RMS terms where more progress was made with weighting towards  error to counteract observed larger tendency for error in this term) and learning rates as training progressed.

Other loss functions, such as RMS in  only, were once again investigated. This produced good losses on the order of , however on further inspection it was found that these resulted from a finding a false minima of , highlighting the complexity of the embedding space in representing physical systems. This also underlined the utility of the previous direct embedding space optimisation done prior in informing us about the behaviour of the loss function itself.

By the training's conclusion we obtained an RMS error of  in  and  in . This model was able to predict systems with some accuracy, clearly able to fit to certain features as shown in fig:model-prediction.


      A comparison of the behaviour of a Lagrangian embedding predicted by a PINN. True embedding, , predicted embedding, .



Discussion



We have presented a number of results on the performance characteristics of the  of the  method when applied to physical problems which warrant additional discussion to place them in context.

Starting with the effect of changing the method order , as presented in fig:dho-r-runtime, the roughly constant behaviour observed and expected to continue for higher values of  as this reflects very little change in the size of the underlying work (due to data-size and calculation cost) involved in the integration process. This coupled with the ability to split long integration runs (high  values) into multiple successive runs means that with careful management of errors, we should be able to the degradation to linear performance observed in fig:dho-n-runtime for  where currently internal arrays become greater than 1Gb in size possibly hitting JAX de-optimisation.

It is important to note however that in the high order domain  the computation of the derivative matrix, given in Equation eq:dij, begins to fail as we encounter issues with the float64 floating point precision used in JAX calculations. Unfortunately, this is unavoidable as it is the maximum precision offered by JAX. Nonetheless, this limitation can be mitigated, whilst retaining the desired error characteristics, by employing alternative strategies, such as reducing the time step or using a higher-order method with a lower value.

Overall however this approach is made more fruitful by noting that we can avoid the fixed cost entirely if we reuse the same of form non-conservative Lagrangian. This allows us to vary not only initial conditions, for example exploring the same system in different circumstances or restarting after multiple runs, but also sample across a range of physical parameters (such as those parameterising the DHO 3 embedding as defined in Equation eq:dho-sys-embedding). In particular this scaling without fixed costs applies effectively to systems composed of many repeated sub-systems such as field theory or molecular dynamics simulations.

Further, as shown in fig:dho-n-runtime, we observe a transient increase in runtime over the range . This phenomenon is repeatable and its origin is uncertain. However, informed by the size of the quadrature array being , we suspect that this behaviour is due to optimisation cliffs in the JAX internal code as we transition between two internal implementations optimised for small vs large array sizes. Finally, it is important to consider that for less trivial systems, where evaluating the Lagrangian may have its own performance impacts, may require further analysis and optimisation. This class of complexity management however is a well studied and understood trade off.

Looking forwards, we noted that the use of fixed time steps in the method limits our ability to maintain energy and momentum fractional error bounds. Moving to an adaptive time step approach would be a promising avenue for improvement in future work.

PINN, Current and Future

The trained neural network represents a promising first step in predicting non-conservative Lagrangians from observational data successfully capturing the general characteristics of the data.
During our experiments, we frequently observed predicted embeddings with values of roughly correct proportion, but off by a constant factor. This suggests that the 4 parameter embedding or other normalisation method may have been useful as the system was unable to move past this local minima in phase space, representing a sufficiently similar physical system.
This poses questions on the handling of degeneracy in our Lagrangian space where multiple Lagrangians can result in the same physical system, especially within the image of any chosen embedding function, as this degeneracy will present issues when optimising within the space.

Furthermore, we noted that the model was substantially more accurate with comparatively high masses. We theorise that this may be due to difficulties in fitting larger values of , possibly indicating the need for a larger and more varied dataset. It is important to acknowledge that this was a simplified test network, and we anticipate that future work will achieve improvements by utilising a larger parameter count and datasets.

To enhance the model's performance, incorporating physically known a priori information into the loss function may be beneficial. For example, a stricter restriction on non-negativity for the DHO 3 parameters could be implemented by first taking the absolute value of the outputs as inputs to the Slimpletic Integrator. However, this approach presents a trade-off, as we specialise towards particular physical systems where we have more physical knowledge, but create models that are less widely applicable. Identifying physical laws that can be enforced in the Lagrangian would be a fruitful avenue for future research.

In the same vein, a suitably trained neural network could be constructed using this method to identify symmetries in observational data or recognise patterns it has previously been exposed to in new observational data. This suggests potential applications in domains such as astrophysics, where there is a large availability of data to train a model, but also difficulties in obtaining a complete dataset for any one experiment.

Conclusion

In this paper we have put forward a more optimal implementation of the Slimpletic Integrator, which is capable of scaling to large scale physical simulations while preserving bounded fractional error in the energy. We have shown this implementation has applications in the construction of a loss function for physics informed neural networks, and presented a simple model which shows that this approach has promise for identifying non-conservative Lagrangians from observational data. More work is required to explore better encodings of physical invariants in the loss function and model structure.



My gratitude is given to my project partner Aengus Roberts who was a guiding light when dealing with the complexities of neural networks, and who spent many hours designing, training and monitoring the network. Without him this would not have been possible. I would also like to thank Dr David Tsang for his constant support and discussion as we have moved through this project. 

References
iopart-num

