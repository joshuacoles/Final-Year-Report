\section{Method}

\subsection{Example Systems}
% I should reference these but I don't know where

\subsubsection{Damped Harmonic Oscillators}

\subsubsection{PRD}

\subsubsection{Large coupled springs}

\subsection{Improvements to the \SI{} and their Physical Applications}

Initially the existing \texttt{slimpletic} codebase \cite{originalCode}, the \orgimpl{}, was rewritten using the JAX framework, the \updimpl{}. We chose two key metrics: run-time and accuracy to judge our model against the \orgimpl{} and RK4, a well known non-variational integrator \todo{we should have referenced this already}. 

First to ensure that we maintained the error bounds expected for our model (as per \sref{sec:intro-si}) we generated graphs of fractional energy and momentum error for RK4, the \orgimpl{}, and the \updimpl{}, similar to that done in the original paper.

Next we create three test-cases to measure performance of the
integrator and its consequences on experimental capability. These were,

\begin{enumerate}
	\item Modelling the same system for different timespans by varying $\Delta t$ and $N_{\text{iterations}}$.
	\item Modelling the same system across different orders of integration $r$.
	\item Modelling a collection of similar systems, across different initial conditions.
\end{enumerate}

The first two of these are designed to test the scaling behaviours of our model to provide insight into its applicability to larger systems, the third acting as a more realistic test case of its use in actual experimental physics. Finally we simulate a large coupled spring system to show the applicability of the observed scaling laws to physical experiments.

\subsection{Applications to Loss Functions}

Taking this \updimpl{} we then explore its different uses as a loss function for physical systems. To start we consider only the form of the loss function, as well as the embedding function to use.

\todo{I probably want a table for embeddings we use}

While these are distinct components of a learning system, they are heavily linked as ideally we would define the physical components of our loss function primarily in terms of the trajectories, rather than the values of the embedding vector itself. Hence the embedding function and slimpletic integrator must be chained together before the physical loss component to provide it with the trajectories.\todo{This feels like a nice place for a data flow diagram}

We consider a number of choices for these two functions comparing their behaviour near known minima, and testing for convexity by randomly sample pairs of points in the embedding space determine if the inequality,

\begin{equation}
  f\left(\frac{x+y}{2}\right) \le \frac{f(x)+f(y)}{2}
\end{equation}

holds near known minima points. \textbf{OR} To test the value of the second derivative of the function.

We also explore these systems through applying an optimiser directly to the space to determine their suitability.

\todo{Again wouldn't it be cool to talk about symmetries here}

\subsection{Loss Function in the Training of Neural Networks}

Finally we apply various loss function and embedding choices to a stand-in neural network observing how the loss varies across training, and the correctness of the outputs.

This involves generating large datasets of different physical systems within the domain of the chosen embedding.

% PRD?

