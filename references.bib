@article{baldiNeuralNetworksPrincipal1989,
  title = {Neural Networks and Principal Component Analysis: {{Learning}} from Examples without Local Minima},
  shorttitle = {Neural Networks and Principal Component Analysis},
  author = {Baldi, Pierre and Hornik, Kurt},
  year = {1989},
  month = jan,
  journal = {Neural Networks},
  volume = {2},
  number = {1},
  pages = {53--58},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(89)90014-2},
  urldate = {2023-10-09},
  abstract = {We consider the problem of learning from examples in layered linear feed-forward neural networks using optimization methods, such as back propagation, with respect to the usual quadratic error function E of the connection weights. Our main result is a complete description of the landscape attached to E in terms of principal component analysis. We show that E has a unique minimum corresponding to the projection onto the subspace generated by the first principal vectors of a covariance matrix associated with the training patterns. All the additional critical points of E are saddle points (corresponding to projections onto subspaces generated by higher order vectors). The auto-associative case is examined in detail. Extensions and implications for the learning algorithms are discussed.},
  keywords = {Back propagation,Learning,Neural networks,Principal component analysis},
  file = {/Users/joshuacoles/Zotero/storage/LE7LJSXG/Baldi and Hornik - 1989 - Neural networks and principal component analysis Learning from examples without local minima.pdf;/Users/joshuacoles/Zotero/storage/EYA73KZ7/0893608089900142.html}
}

@article{bischofMultiObjectiveLossBalancing2021,
  title = {Multi-{{Objective Loss Balancing}} for {{Physics-Informed Deep Learning}}},
  author = {Bischof, Rafael and Kraus, Michael},
  year = {2021},
  eprint = {2110.09813},
  primaryclass = {cs},
  doi = {10.13140/RG.2.2.20057.24169},
  urldate = {2024-04-16},
  abstract = {Physics-Informed Neural Networks (PINN) are algorithms from deep learning leveraging physical laws by including partial differential equations together with a respective set of boundary and initial conditions as penalty terms into their loss function. In this work, we observe the significant role of correctly weighting the combination of multiple competitive loss functions for training PINNs effectively. To this end, we implement and evaluate different methods aiming at balancing the contributions of multiple terms of the PINNs loss function and their gradients. After reviewing of three existing loss scaling approaches (Learning Rate Annealing, GradNorm and SoftAdapt), we propose a novel self-adaptive loss balancing scheme for PINNs named {\textbackslash}emph\{ReLoBRaLo\} (Relative Loss Balancing with Random Lookback). We extensively evaluate the performance of the aforementioned balancing schemes by solving both forward as well as inverse problems on three benchmark PDEs for PINNs: Burgers' equation, Kirchhoff's plate bending equation and Helmholtz's equation. The results show that ReLoBRaLo is able to consistently outperform the baseline of existing scaling methods in terms of accuracy, while also inducing significantly less computational overhead.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/joshuacoles/Zotero/storage/U8ZN4ZET/Bischof and Kraus - 2021 - Multi-Objective Loss Balancing for Physics-Informed Deep Learning.pdf;/Users/joshuacoles/Zotero/storage/QA7TDTGC/2110.html}
}

@article{brouwerAccumulationErrorsNumerical1937,
  title = {On the Accumulation of Errors in Numerical Integration},
  author = {Brouwer, Dirk},
  year = {1937},
  month = oct,
  journal = {The Astronomical Journal},
  volume = {46},
  pages = {149--153},
  publisher = {IOP},
  issn = {0004-6256},
  doi = {10.1086/105423},
  urldate = {2024-04-16},
  annotation = {ADS Bibcode: 1937AJ.....46..149B}
}

@article{chenAutomatedDiscoveryFundamental2022,
  title = {Automated Discovery of Fundamental Variables Hidden in Experimental Data},
  author = {Chen, Boyuan and Huang, Kuang and Raghupathi, Sunand and Chandratreya, Ishaan and Du, Qiang and Lipson, Hod},
  year = {2022},
  month = jul,
  journal = {Nature Computational Science},
  volume = {2},
  number = {7},
  pages = {433--442},
  issn = {2662-8457},
  doi = {10.1038/s43588-022-00281-6},
  urldate = {2023-07-04},
  langid = {english},
  file = {/Users/joshuacoles/Zotero/storage/MKUFKXHM/s43588-022-00281-6.pdf;/Users/joshuacoles/Zotero/storage/WQI7593I/Supplementary Discussion.pdf}
}

@misc{daubechiesIterativeThresholdingAlgorithm2003,
  title = {An Iterative Thresholding Algorithm for Linear Inverse Problems with a Sparsity Constraint},
  author = {Daubechies, Ingrid and Defrise, Michel and De Mol, Christine},
  year = {2003},
  month = nov,
  number = {arXiv:math/0307152},
  eprint = {math/0307152},
  publisher = {arXiv},
  doi = {10.48550/arXiv.math/0307152},
  urldate = {2024-04-16},
  abstract = {We consider linear inverse problems where the solution is assumed to have a sparse expansion on an arbitrary pre-assigned orthonormal basis. We prove that replacing the usual quadratic regularizing penalties by weighted l{\textasciicircum}p-penalties on the coefficients of such expansions, with 1 {$<$} or = p {$<$} or =2, still regularizes the problem. If p {$<$} 2, regularized solutions of such l{\textasciicircum}p-penalized problems will have sparser expansions, with respect to the basis under consideration. To compute the corresponding regularized solutions we propose an iterative algorithm that amounts to a Landweber iteration with thresholding (or nonlinear shrinkage) applied at each iteration step. We prove that this algorithm converges in norm. We also review some potential applications of this method.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Functional Analysis,Mathematics - Numerical Analysis},
  file = {/Users/joshuacoles/Zotero/storage/GS8ZXGYT/Daubechies et al. - 2003 - An iterative thresholding algorithm for linear inverse problems with a sparsity constraint.pdf;/Users/joshuacoles/Zotero/storage/LNZ62T2J/0307152.html}
}

@book{devriesFirstCourseComputational2011,
  title = {A First Course in Computational Physics},
  author = {DeVries, Paul L. and Hasbun, Javier Ernesto},
  year = {2011},
  edition = {2. ed},
  publisher = {{Jones and Bartlett Publ}},
  address = {Sudbury, Mass.},
  isbn = {978-0-7637-7314-4},
  langid = {english}
}

@article{farrVariationalIntegratorsGravitational2007,
  title = {Variational {{Integrators}} for the {{Gravitational N-Body Problem}}},
  author = {Farr, Will M. and Bertschinger, Edmund},
  year = {2007},
  month = jul,
  journal = {The Astrophysical Journal},
  volume = {663},
  number = {2},
  pages = {1420},
  publisher = {IOP Publishing},
  issn = {0004-637X},
  doi = {10.1086/518641},
  urldate = {2023-10-23},
  langid = {english},
  file = {/Users/joshuacoles/Zotero/storage/A7KRUSUI/Farr and Bertschinger - 2007 - Variational Integrators for the Gravitational N-Body Problem.pdf}
}

@article{floryanDatadrivenDiscoveryIntrinsic2022,
  title = {Data-Driven Discovery of Intrinsic Dynamics},
  author = {Floryan, Daniel and Graham, Michael D.},
  year = {2022},
  month = dec,
  journal = {Nature Machine Intelligence},
  volume = {4},
  number = {12},
  pages = {1113--1120},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-022-00575-4},
  urldate = {2023-10-10},
  abstract = {Dynamical models underpin our ability to understand and predict the behaviour of natural systems. Whether dynamical models are developed from first-principles derivations or from observational data, they are predicated on our choice of state variables. The choice of state variables is driven by convenience and intuition, and, in data-driven cases, the observed variables are often chosen to be the state variables. The dimensionality of these variables (and consequently the dynamical models) can be arbitrarily large, obscuring the underlying behaviour of the system. In truth these variables are often highly redundant and the system is driven by a much smaller set of latent intrinsic variables. In this study we combine the mathematical theory of manifolds with the representational capacity of neural networks to develop a method that learns a system's intrinsic state variables directly from time-series data, as well as predictive models for their dynamics. What distinguishes our method is its ability to reduce data to the intrinsic dimensionality of the nonlinear manifold they live on. This ability is enabled by the concepts of charts and atlases from the theory of manifolds, whereby a manifold is represented by a collection of patches that are sewn together---a necessary representation to attain intrinsic dimensionality. We demonstrate this approach on several high-dimensional systems with low-dimensional behaviour. The resulting framework provides the ability to develop dynamical models of the lowest possible dimension, capturing the essence of a system.},
  copyright = {2022 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Computational science,Scientific data},
  file = {/Users/joshuacoles/Zotero/storage/7IZTFTA6/Floryan and Graham - 2022 - Data-driven discovery of intrinsic dynamics.pdf}
}

@article{frostigCompilingMachineLearning,
  title = {Compiling Machine Learning Programs via High-Level Tracing},
  author = {Frostig, Roy and Johnson, Matthew James and Leary, Chris},
  abstract = {We describe JAX, a domain-specific tracing JIT compiler for generating high-performance accelerator code from pure Python and Numpy machine learning programs. JAX uses the XLA compiler infrastructure to generate optimized code for the program subroutines that are most favorable for acceleration, and these optimized subroutines can be called and orchestrated by arbitrary Python. Because the system is fully compatible with Autograd, it allows forward- and reverse-mode automatic differentiation of Python functions to arbitrary order. Because JAX supports structured control flow, it can generate code for sophisticated machine learning algorithms while maintaining high performance. We show that by combining JAX with Autograd and Numpy we get an easily programmable and highly performant ML system that targets CPUs, GPUs, and TPUs, capable of scaling to multi-core Cloud TPUs.},
  langid = {english},
  file = {/Users/joshuacoles/Zotero/storage/B9PUSPKW/Frostig et al. - Compiling machine learning programs via high-level tracing.pdf}
}

@article{galleyClassicalMechanicsNonconservative2013,
  title = {Classical {{Mechanics}} of {{Nonconservative Systems}}},
  author = {Galley, Chad R.},
  year = {2013},
  month = apr,
  journal = {Physical Review Letters},
  volume = {110},
  number = {17},
  pages = {174301},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.110.174301},
  urldate = {2023-10-10},
  langid = {english},
  file = {/Users/joshuacoles/Zotero/storage/NSB889QC/Galley - 2013 - Classical Mechanics of Nonconservative Systems.pdf}
}

@article{galleyPrincipleStationaryNonconservative2014,
  title = {The Principle of Stationary Nonconservative Action for Classical Mechanics and Field Theories},
  author = {Galley, Chad R. and Tsang, David and Stein, Leo C.},
  year = {2014},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1412.3082},
  urldate = {2023-10-10},
  abstract = {We further develop a recently introduced variational principle of stationary action for problems in nonconservative classical mechanics and extend it to classical field theories. The variational calculus used is consistent with an initial value formulation of physical problems and allows for time-irreversible processes, such as dissipation, to be included at the level of the action. In this formalism, the equations of motion are generated by extremizing a nonconservative action \${\textbackslash}mathcal\{S\}\$, which is a functional of a doubled set of degrees of freedom. The corresponding nonconservative Lagrangian contains a potential \$K\$ which generates nonconservative forces and interactions. Such a nonconservative potential can arise in several ways, including from an open system interacting with inaccessible degrees of freedom or from integrating out or coarse-graining a subset of variables in closed systems. We generalize Noether's theorem to show how Noether currents are modified and no longer conserved when \$K\$ is non-vanishing. Consequently, the nonconservative aspects of a physical system are derived solely from \$K\$. We show how to use the formalism with examples of nonconservative actions for discrete systems including forced damped harmonic oscillators, radiation reaction on an accelerated charge, and RLC circuits. We present examples for nonconservative classical field theories. Our approach naturally allows for irreversible thermodynamic processes to be included in an unconstrained variational principle. We present the nonconservative action for a Navier-Stokes fluid including the effects of viscous dissipation and heat diffusion, as well as an action that generates the Maxwell model for viscoelastic materials, which can be easily generalized to more realistic rheological models. We show that the nonconservative action can be derived as the classical limit of a more complete quantum theory.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Fluid Dynamics (physics.flu-dyn),FOS: Physical sciences,High Energy Astrophysical Phenomena (astro-ph.HE),Mathematical Physics (math-ph),Plasma Physics (physics.plasm-ph),Statistical Mechanics (cond-mat.stat-mech)},
  file = {/Users/joshuacoles/Zotero/storage/B8PQJDZM/Galley et al. - 2014 - The principle of stationary nonconservative action for classical mechanics and field theories.pdf}
}

@article{gladmanSymplecticIntegratorsLongterm1991,
  title = {Symplectic Integrators for Long-Term Integrations in Celestial Mechanics},
  author = {Gladman, Brett and Duncan, Martin and Candy, Jeff},
  year = {1991},
  journal = {Celestial Mechanics and Dynamical Astronomy},
  volume = {52},
  number = {3},
  pages = {221--240},
  issn = {0923-2958, 1572-9478},
  doi = {10.1007/BF00048485},
  urldate = {2023-11-12},
  langid = {english},
  file = {/Users/joshuacoles/Zotero/storage/W479CLGT/Gladman et al. - 1991 - Symplectic integrators for long-term integrations in celestial mechanics.pdf}
}

@book{goldsteinClassicalMechanics2000,
  title = {Classical Mechanics},
  author = {Goldstein, Herbert},
  year = {2000},
  series = {Addison-{{Wesley}} Series in Physics},
  edition = {2. ed., 32. [Nachdr.]},
  publisher = {Addison-Wesley},
  address = {Reading, Mass.},
  isbn = {978-0-201-02918-5},
  langid = {english}
}

@article{graySymplecticIntegratorsLarge1994,
  title = {Symplectic Integrators for Large Scale Molecular Dynamics Simulations: {{A}} Comparison of Several Explicit Methods},
  shorttitle = {Symplectic Integrators for Large Scale Molecular Dynamics Simulations},
  author = {Gray, Stephen K. and Noid, Donald W. and Sumpter, Bobby G.},
  year = {1994},
  month = sep,
  journal = {The Journal of Chemical Physics},
  volume = {101},
  number = {5},
  pages = {4062--4072},
  issn = {0021-9606},
  doi = {10.1063/1.467523},
  urldate = {2024-03-29},
  abstract = {We test the suitability of a variety of explicit symplectic integrators for molecular dynamics calculations on Hamiltonian systems. These integrators are extremely simple algorithms with low memory requirements, and appear to be well suited for large scale simulations. We first apply all the methods to a simple test case using the ideas of Berendsen and van Gunsteren. We then use the integrators to generate long time trajectories of a 1000 unit polyethylene chain. Calculations are also performed with two popular but nonsymplectic integrators. The most efficient integrators of the set investigated are deduced. We also discuss certain variations on the basic symplectic integration technique.},
  file = {/Users/joshuacoles/Zotero/storage/9T5ZBPNK/Gray et al. - 1994 - Symplectic integrators for large scale molecular dynamics simulations A comparison of several expli.pdf;/Users/joshuacoles/Zotero/storage/3XUMWNNK/Symplectic-integrators-for-large-scale-molecular.html}
}

@article{hochreiterLongShortTermMemory1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1997.9.8.1735},
  urldate = {2024-04-22},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  langid = {english}
}

@article{hornikMultilayerFeedforwardNetworks1989,
  title = {Multilayer Feedforward Networks Are Universal Approximators},
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  year = {1989},
  month = jan,
  journal = {Neural Networks},
  volume = {2},
  number = {5},
  pages = {359--366},
  issn = {0893-6080},
  doi = {10.1016/0893-6080(89)90020-8},
  urldate = {2024-04-16},
  abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators.},
  keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
  file = {/Users/joshuacoles/Zotero/storage/694RNBLD/0893608089900208.html}
}

@misc{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  number = {arXiv:1412.6980},
  eprint = {1412.6980},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1412.6980},
  urldate = {2024-04-19},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/joshuacoles/Zotero/storage/ZL9RMCW6/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/Users/joshuacoles/Zotero/storage/24PUGYK9/1412.html}
}

@article{luDeepXDEDeepLearning2021,
  title = {{{DeepXDE}}: {{A Deep Learning Library}} for {{Solving Differential Equations}}},
  shorttitle = {{{DeepXDE}}},
  author = {Lu, Lu and Meng, Xuhui and Mao, Zhiping and Karniadakis, George Em},
  year = {2021},
  month = jan,
  journal = {SIAM Review},
  volume = {63},
  number = {1},
  pages = {208--228},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0036-1445},
  doi = {10.1137/19M1274067},
  urldate = {2024-04-16},
  abstract = {Uncertainty quantification (UQ) in machine learning is currently drawing increasing research interest, driven by the rapid deployment of deep neural networks across different fields, such as computer vision and natural language processing, and by the need for reliable tools in risk-sensitive applications. Recently, various machine learning models have also been developed to tackle problems in the field of scientific computing with applications to computational science and engineering (CSE). Physics-informed neural networks and deep operator networks are two such models for solving partial differential equations (PDEs) and learning operator mappings, respectively. In this regard, a comprehensive study of UQ methods tailored specifically for scientific machine learning (SciML) models has been provided in [A. F. Psaros et al., J. Comput. Phys., 477 (2023), art. 111902]. Nevertheless, and despite their theoretical merit, implementations of these methods are not straightforward, especially in large-scale CSE applications, hindering their broad adoption in both research and industry settings. In this paper, we present an open-source Python library ({\u u}lhttps://github.com/Crunch-UQ4MI), termed NeuralUQ and accompanied by an educational tutorial, for employing UQ methods for SciML in a convenient and structured manner. The library, designed for both educational and research purposes, supports multiple modern UQ methods and SciML models. It is based on a succinct workflow and facilitates flexible employment and easy extensions by the users. We first present a tutorial of NeuralUQ and subsequently demonstrate its applicability and efficiency in four diverse examples, involving dynamical systems and high-dimensional parametric and time-dependent PDEs.}
}

@article{mengCompositeNeuralNetwork2020,
  title = {A Composite Neural Network That Learns from Multi-Fidelity Data: {{Application}} to Function Approximation and Inverse {{PDE}} Problems},
  shorttitle = {A Composite Neural Network That Learns from Multi-Fidelity Data},
  author = {Meng, Xuhui and Karniadakis, George Em},
  year = {2020},
  month = jan,
  journal = {Journal of Computational Physics},
  volume = {401},
  pages = {109020},
  issn = {0021-9991},
  doi = {10.1016/j.jcp.2019.109020},
  urldate = {2024-04-16},
  abstract = {Currently the training of neural networks relies on data of comparable accuracy but in real applications only a very small set of high-fidelity data is available while inexpensive lower fidelity data may be plentiful. We propose a new composite neural network (NN) that can be trained based on multi-fidelity data. It is comprised of three NNs, with the first NN trained using the low-fidelity data and coupled to two high-fidelity NNs, one with activation functions and another one without, in order to discover and exploit nonlinear and linear correlations, respectively, between the low-fidelity and the high-fidelity data. We first demonstrate the accuracy of the new multi-fidelity NN for approximating some standard benchmark functions but also a 20-dimensional function that is not easy to approximate with other methods, e.g. Gaussian process regression. Subsequently, we extend the recently developed physics-informed neural networks (PINNs) to be trained with multi-fidelity data sets (MPINNs). MPINNs contain four fully-connected neural networks, where the first one approximates the low-fidelity data, while the second and third construct the correlation between the low- and high-fidelity data and produce the multi-fidelity approximation, which is then used in the last NN that encodes the partial differential equations (PDEs). Specifically, by decomposing the correlation into a linear and nonlinear part, the present model is capable of learning both the linear and complex nonlinear correlations between the low- and high-fidelity data adaptively. By training the MPINNs, we can: (1) obtain the correlation between the low- and high-fidelity data, (2) infer the quantities of interest based on a few scattered data, and (3) identify the unknown parameters in the PDEs. In particular, we employ the MPINNs to learn the hydraulic conductivity field for unsaturated flows as well as the reactive models for reactive transport. The results demonstrate that MPINNs can achieve relatively high accuracy based on a very small set of high-fidelity data. Despite the relatively low dimension and limited number of fidelities (two-fidelity levels) for the benchmark problems in the present study, the proposed model can be readily extended to very high-dimensional regression and classification problems involving multi-fidelity data.},
  keywords = {Adversarial data,Multi-fidelity,Physics-informed neural networks,Porous media,Reactive transport},
  file = {/Users/joshuacoles/Zotero/storage/C89VA9P6/S0021999119307260.html}
}

@article{mittalAnalysisPeriodicOrbits2020,
  title = {The Analysis of Periodic Orbits Generated by {{Lagrangian}} Solutions of the Restricted Three-Body Problem with Non-Spherical Primaries},
  author = {Mittal, Amit and Suraj, Md Sanam and Aggarwal, Rajiv},
  year = {2020},
  month = jan,
  journal = {New Astronomy},
  volume = {74},
  pages = {101287},
  issn = {13841076},
  doi = {10.1016/j.newast.2019.101287},
  urldate = {2024-04-21},
  abstract = {The present paper deals with the periodic orbits generated by Lagrangian solutions of the restricted three-body problem when both the primaries are oblate bodies. We have illustrated the periodic orbits for different values of {$\mu$}, h, {$\sigma$}1 and {$\sigma$}2 (h is energy constant, {$\mu$} is mass ratio of the two primaries, {$\sigma$}1 and {$\sigma$}2 are oblateness factors). These orbits have been determined by giving displacements along the tangent and normal to the mobile coordinates as defined by (Karimov and Sokolsky, 1989). We have applied the predictor-corrector algorithm to construct the periodic orbits in an attempt to unveil the effect of oblateness of the primaries by taking the fixed values of parameters {$\mu$}, h, {$\sigma$}1 and {$\sigma$}2.},
  langid = {english},
  file = {/Users/joshuacoles/Zotero/storage/A7T7A9X5/Mittal et al. - 2020 - The analysis of periodic orbits generated by Lagrangian solutions of the restricted three-body probl.pdf}
}

@inproceedings{mohammadiPerformanceNoisyNesterov2019,
  title = {Performance of Noisy {{Nesterov}}'s Accelerated Method for Strongly Convex Optimization Problems},
  booktitle = {2019 {{American Control Conference}} ({{ACC}})},
  author = {Mohammadi, Hesameddin and Razaviyayn, Meisam and Jovanovi{\'c}, Mihailo R.},
  year = {2019},
  month = jul,
  pages = {3426--3431},
  issn = {2378-5861},
  doi = {10.23919/ACC.2019.8814680},
  urldate = {2024-04-16},
  abstract = {We study the performance of noisy gradient descent and Nesterov's accelerated methods for strongly convex objective functions with Lipschitz continuous gradients. The steady-state second-order moment of the error in the iterates is analyzed when the gradient is perturbed by an additive white noise with zero mean and identity covariance. For any given condition number {$\kappa$}, we derive explicit upper bounds on noise amplification that only depend on {$\kappa$} and the problem size. We use quadratic objective functions to derive lower bounds and to demonstrate that the upper bounds are tight up to a constant factor. The established upper bound for Nesterov's accelerated method is larger than the upper bound for gradient descent by a factor of {\textsurd}{$\kappa$}. This gap identifies a fundamental tradeoff that comes with acceleration in the presence of stochastic uncertainties in the gradient evaluation.},
  keywords = {Accelerated first-order algorithms,control for optimization,convex optimization,integral quadratic constraints,linear matrix inequalities,Nesterov's method,noise amplification,second-order moments,semidefinite programming},
  file = {/Users/joshuacoles/Zotero/storage/RZ8Z2ADA/Mohammadi et al. - 2019 - Performance of noisy Nesterov's accelerated method for strongly convex optimization problems.pdf;/Users/joshuacoles/Zotero/storage/PRVJNCDE/8814680.html}
}

@article{noetherInvariantVariationProblems1971,
  title = {Invariant {{Variation Problems}}},
  author = {Noether, Emmy and Tavel, M. A.},
  year = {1971},
  month = jan,
  journal = {Transport Theory and Statistical Physics},
  volume = {1},
  number = {3},
  eprint = {physics/0503066},
  pages = {186--207},
  issn = {0041-1450, 1532-2424},
  doi = {10.1080/00411457108231446},
  urldate = {2023-11-12},
  abstract = {The problems in variation here concerned are such as to admit a continuous group (in Lie's sense); the conclusions that emerge from the corresponding differential equations find their most general expression in the theorems formulated in Section 1 and proved in following sections. Concerning these differential equations that arise from problems of variation, far more precise statements can be made than about arbitrary differential equations admitting of a group, which are the subject of Lie's researches. What is to follow, therefore, represents a combination of the methods of the formal calculus of variations with those of Lie's group theory. For special groups and problems in variation, this combination of methods is not new; I may cite Hamel and Herglotz for special finite groups, Lorentz and his pupils (for instance Fokker), Weyl and Klein for special infinite groups. Especially Klein's second Note and the present developments have been mutually influenced by each other, in which regard I may refer to the concluding remarks of Klein's Note.},
  archiveprefix = {arxiv},
  keywords = {Physics - History and Philosophy of Physics},
  file = {/Users/joshuacoles/Zotero/storage/CIUMFWS5/Noether and Tavel - 1971 - Invariant Variation Problems.pdf;/Users/joshuacoles/Zotero/storage/IHGFJM72/0503066.html}
}

@article{okumuraExplicitSymplecticIntegrators2007,
  title = {Explicit Symplectic Integrators of Molecular Dynamics Algorithms for Rigid-Body Molecules in the Canonical, Isobaric-Isothermal, and Related Ensembles},
  author = {Okumura, Hisashi and Itoh, Satoru G. and Okamoto, Yuko},
  year = {2007},
  month = feb,
  journal = {The Journal of Chemical Physics},
  volume = {126},
  number = {8},
  pages = {084103},
  issn = {0021-9606, 1089-7690},
  doi = {10.1063/1.2434972},
  urldate = {2024-03-29},
  abstract = {The authors propose explicit symplectic integrators of molecular dynamics (MD) algorithms for rigid-body molecules in the canonical and isobaric-isothermal ensembles. They also present a symplectic algorithm in the constant normal pressure and lateral surface area ensemble and that combined with the Parrinello-Rahman algorithm. Employing the symplectic integrators for MD algorithms, there is a conserved quantity which is close to Hamiltonian. Therefore, they can perform a MD simulation more stably than by conventional nonsymplectic algorithms. They applied this algorithm to a TIP3P pure water system at 300K and compared the time evolution of the Hamiltonian with those by the nonsymplectic algorithms. They found that the Hamiltonian was conserved well by the symplectic algorithm even for a time step of 4fs. This time step is longer than typical values of 0.5--2fs which are used by the conventional nonsymplectic algorithms.},
  langid = {english},
  file = {/Users/joshuacoles/Zotero/storage/5CF4G3TZ/Okumura et al. - 2007 - Explicit symplectic integrators of molecular dynamics algorithms for rigid-body molecules in the can.pdf}
}

@article{psarosMetalearningPINNLoss2022,
  title = {Meta-Learning {{PINN}} Loss Functions},
  author = {Psaros, Apostolos F and Kawaguchi, Kenji and Karniadakis, George Em},
  year = {2022},
  month = jun,
  journal = {Journal of Computational Physics},
  volume = {458},
  pages = {111121},
  issn = {0021-9991},
  doi = {10.1016/j.jcp.2022.111121},
  urldate = {2024-04-16},
  abstract = {We propose a meta-learning technique for offline discovery of physics-informed neural network (PINN) loss functions. We extend earlier works on meta-learning, and develop a gradient-based meta-learning algorithm for addressing diverse task distributions based on parametrized partial differential equations (PDEs) that are solved with PINNs. Furthermore, based on new theory we identify two desirable properties of meta-learned losses in PINN problems, which we enforce by proposing a new regularization method or using a specific parametrization of the loss function. In the computational examples, the meta-learned losses are employed at test time for addressing regression and PDE task distributions. Our results indicate that significant performance improvement can be achieved by using a shared-among-tasks offline-learned loss function even for out-of-distribution meta-testing. In this case, we solve for test tasks that do not belong to the task distribution used in meta-training, and we also employ PINN architectures that are different from the PINN architecture used in meta-training. To better understand the capabilities and limitations of the proposed method, we consider various parametrizations of the loss function and describe different algorithm design options and how they may affect meta-learning performance.},
  keywords = {Meta-learned loss function,Meta-learning,Physics-informed neural networks},
  file = {/Users/joshuacoles/Zotero/storage/3SXRLBTL/Psaros et al. - 2022 - Meta-learning PINN loss functions.pdf;/Users/joshuacoles/Zotero/storage/TSCR3FSL/1-s2.0-S0021999122001838-main.pdf;/Users/joshuacoles/Zotero/storage/EYDDUHCX/S0021999122001838.html}
}

@misc{raissiPhysicsInformedDeep2017,
  title = {Physics {{Informed Deep Learning}} ({{Part II}}): {{Data-driven Discovery}} of {{Nonlinear Partial Differential Equations}}},
  shorttitle = {Physics {{Informed Deep Learning}} ({{Part II}})},
  author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
  year = {2017},
  month = nov,
  number = {arXiv:1711.10566},
  eprint = {1711.10566},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1711.10566},
  urldate = {2024-04-16},
  abstract = {We introduce physics informed neural networks -- neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial differential equations. In this second part of our two-part treatise, we focus on the problem of data-driven discovery of partial differential equations. Depending on whether the available data is scattered in space-time or arranged in fixed temporal snapshots, we introduce two main classes of algorithms, namely continuous time and discrete time models. The effectiveness of our approach is demonstrated using a wide range of benchmark problems in mathematical physics, including conservation laws, incompressible fluid flow, and the propagation of nonlinear shallow-water waves.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Analysis of PDEs,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  file = {/Users/joshuacoles/Zotero/storage/BL27AQG6/1711.html}
}

@misc{schoenholzJAXFrameworkDifferentiable2020,
  title = {{{JAX}}, {{M}}.{{D}}.: {{A Framework}} for {{Differentiable Physics}}},
  shorttitle = {{{JAX}}, {{M}}.{{D}}.},
  author = {Schoenholz, Samuel S. and Cubuk, Ekin D.},
  year = {2020},
  month = dec,
  number = {arXiv:1912.04232},
  eprint = {1912.04232},
  primaryclass = {cond-mat, physics:physics, stat},
  publisher = {arXiv},
  urldate = {2023-12-13},
  abstract = {We introduce JAX MD, a software package for performing differentiable physics simulations with a focus on molecular dynamics. JAX MD includes a number of physics simulation environments, as well as interaction potentials and neural networks that can be integrated into these environments without writing any additional code. Since the simulations themselves are differentiable functions, entire trajectories can be differentiated to perform meta-optimization. These features are built on primitive operations, such as spatial partitioning, that allow simulations to scale to hundreds-of-thousands of particles on a single GPU. These primitives are flexible enough that they can be used to scale up workloads outside of molecular dynamics. We present several examples that highlight the features of JAX MD including: integration of graph neural networks into traditional simulations, meta-optimization through minimization of particle packings, and a multi-agent flocking simulation. JAX MD is available at www.github.com/google/jax-md.},
  archiveprefix = {arxiv},
  keywords = {Condensed Matter - Materials Science,Condensed Matter - Soft Condensed Matter,Physics - Computational Physics,Statistics - Machine Learning},
  file = {/Users/joshuacoles/Zotero/storage/KB2LPVTM/Schoenholz and Cubuk - 2020 - JAX, M.D. A Framework for Differentiable Physics.pdf;/Users/joshuacoles/Zotero/storage/48A96IFD/1912.html}
}

@book{sraOptimizationMachineLearning2012,
  title = {Optimization for Machine Learning},
  editor = {Sra, Suvrit and Nowozin, Sebastian and Wright, Stephen J.},
  year = {2012},
  series = {Neural Information Processing Series},
  publisher = {MIT Press},
  address = {Cambridge, Mass},
  isbn = {978-0-262-01646-9},
  lccn = {Q325.5 .O65 2012},
  keywords = {Machine learning,Mathematical models,Mathematical optimization},
  annotation = {OCLC: ocn701493361},
  file = {/Users/joshuacoles/Zotero/storage/I9S5MDXH/(Neural Information Processing series) Suvrit Sra, Sebastian Nowozin, Stephen J. Wright - Optimization for Machine Learning-The MIT Press (2011).pdf}
}

@article{struttGeneralTheoremsRelating1871,
  title = {Some {{General Theorems}} Relating to {{Vibrations}}},
  author = {Strutt, J. W.},
  year = {1871},
  month = nov,
  journal = {Proceedings of the London Mathematical Society},
  volume = {s1-4},
  number = {1},
  pages = {357--368},
  issn = {00246115},
  doi = {10.1112/plms/s1-4.1.357},
  urldate = {2024-04-15},
  copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
  langid = {english},
  file = {/Users/joshuacoles/Zotero/storage/A38QPXAJ/Strutt - 1871 - Some General Theorems relating to Vibrations.pdf}
}

@misc{tenachiDeepSymbolicRegression2023a,
  title = {Deep Symbolic Regression for Physics Guided by Units Constraints: Toward the Automated Discovery of Physical Laws},
  shorttitle = {Deep Symbolic Regression for Physics Guided by Units Constraints},
  author = {Tenachi, Wassim and Ibata, Rodrigo and Diakogiannis, Foivos I.},
  year = {2023},
  month = oct,
  number = {arXiv:2303.03192},
  eprint = {2303.03192},
  primaryclass = {astro-ph, physics:physics},
  publisher = {arXiv},
  urldate = {2023-11-21},
  abstract = {Symbolic Regression is the study of algorithms that automate the search for analytic expressions that fit data. While recent advances in deep learning have generated renewed interest in such approaches, the development of symbolic regression methods has not been focused on physics, where we have important additional constraints due to the units associated with our data. Here we present \${\textbackslash}Phi\$-SO, a Physical Symbolic Optimization framework for recovering analytical symbolic expressions from physics data using deep reinforcement learning techniques by learning units constraints. Our system is built, from the ground up, to propose solutions where the physical units are consistent by construction. This is useful not only in eliminating physically impossible solutions, but because the "grammatical" rules of dimensional analysis restrict enormously the freedom of the equation generator, thus vastly improving performance. The algorithm can be used to fit noiseless data, which can be useful for instance when attempting to derive an analytical property of a physical model, and it can also be used to obtain analytical approximations to noisy data. We test our machinery on a standard benchmark of equations from the Feynman Lectures on Physics and other physics textbooks, achieving state-of-the-art performance in the presence of noise (exceeding 0.1\%) and show that it is robust even in the presence of substantial (10\%) noise. We showcase its abilities on a panel of examples from astrophysics.},
  archiveprefix = {arxiv},
  keywords = {Astrophysics - Instrumentation and Methods for Astrophysics,Computer Science - Machine Learning,Physics - Computational Physics},
  file = {/Users/joshuacoles/Zotero/storage/UYVIJSX3/Tenachi et al. - 2023 - Deep symbolic regression for physics guided by units constraints toward the automated discovery of .pdf;/Users/joshuacoles/Zotero/storage/8AITZZNA/2303.html}
}

@article{tsangSLIMPLECTICINTEGRATORSVARIATIONAL2015,
  title = {``{{SLIMPLECTIC}}'' {{INTEGRATORS}}: {{VARIATIONAL INTEGRATORS FOR GENERAL NONCONSERVATIVE SYSTEMS}}},
  shorttitle = {``{{SLIMPLECTIC}}'' {{INTEGRATORS}}},
  author = {Tsang, David and Galley, Chad R. and Stein, Leo C. and Turner, Alec},
  year = {2015},
  month = aug,
  journal = {The Astrophysical Journal},
  volume = {809},
  number = {1},
  pages = {L9},
  issn = {2041-8213},
  doi = {10.1088/2041-8205/809/1/L9},
  urldate = {2023-09-28},
  file = {/Users/joshuacoles/Zotero/storage/76E3SVCY/Tsang et al. - 2015 - “SLIMPLECTIC” INTEGRATORS VARIATIONAL INTEGRATORS FOR GENERAL NONCONSERVATIVE SYSTEMS.pdf;/Users/joshuacoles/Zotero/storage/D4YUL9DE/Tsang et al. - 2015 - “SLIMPLECTIC” INTEGRATORS VARIATIONAL INTEGRATORS FOR GENERAL NONCONSERVATIVE SYSTEMS.pdf}
}

@article{tsangVariationalSymplecticIntegrators,
  title = {Variational {{Symplectic Integrators For Velocity Dependent Conservative Potentials And Nearly Integrable Systems}}},
  author = {Tsang, David and Hamilton, Douglas P},
  langid = {english},
  file = {/Users/joshuacoles/Zotero/storage/PUZ8DYXZ/Tsang and Hamilton - VARIATIONAL SYMPLECTIC INTEGRATORS FOR VELOCITY DEPENDENT CONSERVATIVE POTENTIALS AND NEARLY INTEGRA.pdf}
}

@article{tuckermanUnderstandingModernMolecular2000b,
  title = {Understanding {{Modern Molecular Dynamics}}:\, {{Techniques}} and {{Applications}}},
  shorttitle = {Understanding {{Modern Molecular Dynamics}}},
  author = {Tuckerman, Mark E. and Martyna, Glenn J.},
  year = {2000},
  month = jan,
  journal = {The Journal of Physical Chemistry B},
  volume = {104},
  number = {2},
  pages = {159--178},
  publisher = {American Chemical Society},
  issn = {1520-6106},
  doi = {10.1021/jp992433y},
  urldate = {2024-04-23},
  abstract = {Recent advances in molecular dynamics methodology have made it possible to study routinely the microscopic details of chemical processes in the condensed phase using high-speed computers. Thus, it is timely and useful to provide a pedagogical treatment of the theoretical and numerical aspects of modern molecular dynamics simulation techniques and to show several applications that illustrate the capability of these approaches. First, the standard Newtonian or Hamiltonian dynamics based method is presented followed by a discussion of theoretical advances related to non-Hamiltonian molecular dynamics. Examples of non-Hamiltonian molecular dynamics schemes capable of generating the canonical and isothermal-isobaric ensemble are analyzed. Next, the novel Liouville operator factorization approach to numerical integration is reviewed. The power and utility of this new technique are contrasted to more basic methods, particularly, in the development of multiple time scale and non-Hamiltonian integrators. Since the results of molecular dynamics simulations depend on the interparticle interactions employed in the calculations, modern empirical force fields and ab initio molecular dynamics approaches are discussed. An example calculation combining an empirical force field and novel molecular dynamics methods, the mutant T4 lysozyme M61 in water, will be presented. The combination of electronic structure with classical dynamics, the so called ab initio molecular dynamics method, will be described and an application to the structure of liquid ammonia discussed. Last, it will then be shown how the classical molecular dynamics methods can be adapted for quantum calculations using the Feynman path integral formulation of statistical mechanics. An application, employing both path integrals and ab initio molecular dynamics, to an excess proton in water will be presented.},
  file = {/Users/joshuacoles/Zotero/storage/UCZFAFUJ/Tuckerman and Martyna - 2000 - Understanding Modern Molecular Dynamics  Techniques and Applications.pdf}
}

@article{zhongLiePoissonHamiltonJacobiTheory1988,
  title = {Lie-{{Poisson Hamilton-Jacobi}} Theory and {{Lie-Poisson}} Integrators},
  author = {Zhong, Ge and Marsden, Jerrold E.},
  year = {1988},
  month = nov,
  journal = {Physics Letters A},
  volume = {133},
  number = {3},
  pages = {134--139},
  issn = {0375-9601},
  doi = {10.1016/0375-9601(88)90773-6},
  urldate = {2024-04-21},
  abstract = {We present results on numerical integrators that exactly preserve momentum maps and Poisson brackets, thereby inducing integrators that preserve the natural Lie-Poisson structure on the duals of Lie algebras. The techniques are baseda on time-stepping with the generating function obtained as an approximate solution to the Hamilton-Jacobi equation, following ideas of deVogela{\'e}re, Channel,, and Feng. To accomplish this, the Hamilton-Jacobi theory is reduced from T{$\ast$}G to g{$\ast$}, where g is the Lie algebra of a Lie group G. The algorithms exactly preserve any additional conserved quantities in the problem. An explicit algorithm is given for any semi-simple group and in particular for the Euler equation of rigid body dynamics.},
  file = {/Users/joshuacoles/Zotero/storage/M5ACTDA5/Zhong and Marsden - 1988 - Lie-Poisson Hamilton-Jacobi theory and Lie-Poisson integrators.pdf;/Users/joshuacoles/Zotero/storage/SX8VADH7/0375960188907736.html}
}
