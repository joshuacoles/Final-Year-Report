\documentclass[10pt]{iopart}
\include{setup}

\def\SI{Slimpletic Integrator}
\def\SymI{{Sympletic integrator}}
\newcommand{\autodiff}{automatic-differentiation}

\begin{document}
\ioptwocol[{
\title{Iterations on the slimpletic integrator and its applications to physically informed loss functions}
\author{J D Coles$^1$}
\address{$^1$ Department of Physics, University of Bath, Claverton Down, Bath BA2 7AY, UK}
\begin{abstract}
\lipsum[1]
\end{abstract}
}]

\section{Introduction}
% Heres all the knowledge you need to understand shit
% Following DT's SI paper, we start with top line introductions of the main areas of our paper

Neural networks (NNs) and other machine learning (ML) models are becoming an increasingly important part of modern research and scientific application as they show wider application to data intensive problems and make progress where prior computational methods have proved otherwise intractable.

One method of training ML models involves the construction of a loss functions, or equivalently reward functions, which frames the training process as a high-dimensionality optimisation problem. In this setting the various parameters of the model are varied such as to minimise the aggregate loss of the models action over a large collection of inputs, compared to outputs. It is thus the loss function that grounds the model in a given problem. Their construction varies but they exhibit some general characteristics as will be discussed.

% TODO: merge these two paragraphs
% Question: What is a variational vs Sympletic Integration
% See DT1 for good complementary references and explantion here
The \SI{} (SI)\cite{tsangSLIMPLECTICINTEGRATORSVARIATIONAL2015} is a non-conservative extension of the the \SymI{}, a conservative variational integrator, which enables numerical integration of non-conservative systems that exhibits well defined bounds on the error in the energy and other conserved quantities of the system. These are based on the non-conservative action approach developed by Galley\cite{galleyClassicalMechanicsNonconservative2013} and Galley et al.\cite{galleyPrincipleStationaryNonconservative2014}.

\SymI{} are numerical integrators for Hamiltonian systems which have the property of preserving the canonical symplectic 2-form of the system.% Cite: https://en.wikipedia.org/wiki/Symplectic_integrator and DT's unpublished paper.
This makes them widely applicable to physical fields such as orbital dynamics and  molecular dynamics, among others \todo{Cite papers, some from DT2, some of my own} as they will by this nature,  preserve, or near preserve, the constants of motion of a system over a large number of integration steps.


% TODO: This feels a little stilted still. Why cant SymPy use automatic differentiation?
Currently the \SI{} is primarily implemented using computer algebra systems such as SymPy\cite{sympy} which allow for the computation of derivatives and integrals of mathematical expressions. These are general systems and are effective at working with mathematical expressions but pose limitations when seeking to increase computational speed or scale up to larger systems. Automatic-differentiation \todo{do we capitalise both words?} techniques are a collection of computational methods for the determination of the differential of large classes of \enquote{regular} scientific code in an efficient and accurate manner.

% Aside: I want to keep consistent terminology here. I quite like physics based neural networks
In this paper we focus on taking the existing mathematical framework of the \SI{} and adapting it to use more advanced computational methods and making it amenable to \autodiff{} to enable a wide class of new applications. These range from scalability to larger systems, for example in molecular dynamics, to its use as the foundation of a number of physics based loss functions which we will compare for empirical and theoretical suitability, with the goal that these could be used in the creation of machine learning models which encode physical knowledge and insight.

%\begin{enumerate}
%	\item X What is a slimpletic integrator
%	\item X What are they useful for
%	\item What is the need to auto-diff and a higher speed one
%	\item What \PHYSICS does this let us do?
%	\item How do we get to autodiff
%	\item Introduce the physical system which will be the through line of our model.
%\end{enumerate}

\subsection{Non-Conservative Actions}
\label{sec:intro-nc-actions}

\begin{quote}
Noether’s theorem for conservative actions can be shown to generalize to nonconservative systems where the corresponding Noether currents evolve in time due to a non-zero K (Galley et al. 2014). One can show that for continuous symmetries of the conservative action, which remain after discretization, discrete Noether currents will also evolve due to Kd. Thus, translational or rotational symmetries, for example, will generate discrete momenta that evolve accord- ing to Kd, up to round off and bias error (Brouwer 1937; Rein \& Spiegel 2015). Additional error compared to the physical evolution is only due to the discretization of the action.
\\\\
The GGL discretization does not preserve the time-shift symmetry preventing energy evolution from being precisely tracked. However, the fractional energy error tends to be oscillatory and bounded by a resolution and order-dependent constant.7 We will defer more detailed discussion of Noether current evolution to a longer follow-up paper in the interests of space.
\\\\
The resulting slimplectic maps are accurate up to order 2r + 2. For r = 0, where no intermediate steps are used, the quadrature method is the trapezoid rule, and the variational integrator is second-order and equivalent to the Störmer–Verlet “leap-frog” integrator (Wendlandt \& Marsden 1997).
\end{quote}




% Introduce lambda so we can talk about it in the SI Sec

% TODO Where do we put applications of autodiff?
\subsection{The \SI}
\label{sec:intro-si}

% Setup system
% TOOD: Rephrase this to be variational integrators in general

To understand the \SI{} method let us first consider the application of the \SymI{} application to a traditional conservative system\footnote{This explanation takes its path from an unpublished paper by Tsang et al\cite{tsangVariationalSymplecticIntegrators}} with $N$ degrees of freedom represented by some $\vb{q} \in \R^N$. We take that this system is governed by some Lagrangian, $L(\vb{q}, \dot{\vb q}, t) \in \R$ where $\dot{\vb q}$ is the standard time derivative of the degrees of freedom vector $\vb q$.

Hamilton's Principle of least action \cite{goldsteinClassicalMechanics2000} provides that the physically attained path of the system through the phase space\todo{Naming of the different spaces}, $\vb q(t)$, from some initial time $t_i$, to some final time $t_f$, is one that extremises the action integral along that path,

\begin{equation}
\label{eq:action-integral}
	S = \int_{t_i}^{t_f} L(\vb{q}(t), \dot{\vb{q}}(t), t) \rdd t.
\end{equation}

The path that meets this condition is one where the variation $\delta S$ with respect to any variation of the path $\delta \vb q(t)$ is zero, ie that corresponding to a solution of the Euler-Lagrange equations,

\begin{equation}
	\label{eq:euler-lagrange}
	\frac{\p L(\vb q, \vb{\dot q})}{\p q} - \frac{\dd}{\dd t}\frac{\p L(\vb q, \vb{\dot q})}{\p t} = 0.
\end{equation}

% Apply

With this setup we now choose to apply two successive procedures to our extremal path $\vb q(t)$, piecewise breakdown and discretisation with an aim to determine it numerically.

To start we break the trajectory down piecewise into a collection of $M$ sub-paths $\gamma_{n \in [M - 1]}$ where $[A] = \set{0, \dots, A}$, each defined on some portion of the whole timespan $[t_n, t_{n + 1}] \subset [t_i, t_f] \subset \R$ such that they cover it completely with overlaps only at the boundaries of the intervals. These together are such that the there is the correspondence,

\begin{equation}
\label{eq:pw-traj}
	\vb q(t) = \begin{cases}
		\gamma_n(t) &\text{for~} t \in [t_n, t_{n + 1}]
	\end{cases}
\end{equation}

for all $t \in [t_i, t_f]$. These paths define a collection of points $\vb q_{n \in [M]}$ which are their mutual values at the piecewise break points and the initial and final values for $n = 0$ and $n = M$ respectively.

As each piecewise curve constitutes in and of itself a physically attained trajectory we can state that any path $\vb{q}(t)$ which extremises the action for the whole timespan, must also extremise each piecewise portion, and visa-versa. 

This does not bring us closer to a numerical solution directly but allows us to freely split our integral domain as needed without loss of accuracy, and thus limit the error of our numerical-integration method which will necessarily increase with the number of time-steps or their size.

We now turn to the discretisation method itself. This is done using the Galerkin-Gauss-Lobatto quadrature method of order $r \in \N_0$\cite{tsangSLIMPLECTICINTEGRATORSVARIATIONAL2015} which approximates the integral from $t_n$ to $t_{n + 1}$ using the intermediary points

\begin{equation}
  t^{(i)}_n = t_n + (1 + x_i)\frac{\Delta t}{2}
\end{equation}

where $i \in [r + 1]$ and $x_i$ are the ordered roots of the the derivative of the $(r + 1)$th Legendre Polynomial $P_{r + 1}$ and $\Delta t = t_f - t_i$. These allow us to approximate the path of the system within this quadrature using the associated cardinal functions for the GGL quadrature, labelled $\phi(t)$, as \(\vb q(t) = \phi(t) + \Or((\Delta t)^{r + 2})\) \todo{What does DT1 actually mean in this bit??? Bottom of pg2col1}. These provide a suitable approximation for the path derivative $\dot{\vb q}$ by the use the derivative matrix,

\begin{equation}
  D_{ij} = \begin{cases}
  	\dfrac{(r + 1)(r + 2)}{2\Delta t} &i = j = 0 \\\\
  	-\dfrac{(r + 1)(r + 2)}{2\Delta t} & i = j = r + 1 \\\\
  	0 & i = j \land i \notin \set{0, r + 1} \\\\
  	\dfrac{2P_{r + 1}(x_i)}{P_{r+1}(x_j)(x - x_j)\Delta t} & i\neq j
  \end{cases}
\end{equation}

which provides that,

\begin{equation}
  \dot\phi(t_n^{(i)}) = \sum_{j = 0}^{r + 1} D_{ij}q_n^{(j)}
\end{equation}

In turn this allows to express the integral as,

\begin{equation}
\label{eq:discr-action-1}
  \int_{t_n}^{t_{n + 1}} L(\vb q, \dot{\vb q}, t) \dd t \approx \sum_{i = 0}^{r + 1} w_i L(q_{n}^{(i)}, \dot\phi_{n}^{(i)}, t_{n}^{(i)})
\end{equation}

where the label the approximate expression $L_d^n$ and where $w_i$ are the quadrature weights given by

\begin{equation}
  w_i = \frac{\Delta t}{(r + 1)(r + 2)(P_{r + 1}(x_i))^2}
\end{equation}

Equation \eqref{eq:discr-action-1} strung together across the different piecewise sub-trajectories defined in Equation \eqref{eq:pw-traj} gives us the total discretised action of the system, shown in Equation \eqref{eq:discr-action-2}, that sets variational integrators \todo{is this sympl or variational} apart from their \todo{rant about other integrators at the start of this section} more general counterparts. 

\begin{equation}
\label{eq:discr-action-2}
  S_d = \sum_{n = 0}^{M} \sum_{i = 0}^{r + 1} w_i L(q_{n}^{(i)}, \dot\phi_{n}^{(i)}, t_{n}^{(i)})
\end{equation}

From here we then extremise this approximate path with respect to the mutual and interior \todo{have I used this terminology before?} points to obtain the equations of motion of the system as,

\begin{gather}
	\frac{\p L_d^{n-1}}{\p q_n} + \frac{\p L_d^{n}}{\p q_n} = 0 \\
	\label{eq:Ld-interior-eom} \frac{\p L_d^{n}}{\p q^{(i)}_n} = 0
\end{gather}

where $L_d^{n}$ is This first equation can be simplified further into two equivalent definitions for the discrete momentum \(\pi_n\),

\begin{align}
	\label{eq:pi-n} \pi_n &= -\frac{\p L_d^{n}}{\p q_n} \\
	\label{eq:pi-n+1} \pi_{n + 1} &= \frac{\p L_d^{n}}{\p q_{n + 1}}
\end{align}

and a continuity constraint at these mutual overlap points $\vb q_n$ in the same manner as we required for $\vb q_n$ itself. Together Equations \eqref{eq:Ld-interior-eom}, \eqref{eq:pi-n} can be solved to determine the values of $\vb q^{(i)}_n$, from which Equation \eqref{eq:pi-n+1} provides a form for determining $\pi_{n + 1}$.

This form is provided in terms of the traditional conservative Lagrangian $L$ however can be readily adapted to the non-conservative Lagrangian formalism discussed in \sref{sec:intro-nc-actions} by substituting $L$ for the non-conservative Lagrangian $\Lambda$ and $\vb q_n$ for $\vb q_{n, -}$.


%Whereas a traditional numerical integration approach works by discretising the equations of motion that result from the Euler Lagrange equations, Sympletic Integrators work by discretising the action itself. This process is done in two steps.
%
%First we split the path the system will take through phase space into sub-paths \todo{insert maths}, then we apply approximation to these in turn. This first discretisation does not introduce error in itself as it can be seen as per Equation \todo{Equations} that extemalising the action on the overall path is equivalent to extremealising the action across the split paths.

%Our chosen approximation as in DT1 and DT2 is the GGL quadrature. Thi  

\subsection{Physics Informed Neural Networks}
\label{sec:intro-pinn}

\subsection{Loss functions}
\label{sec:intro-lf}

% Slimpletic Integrator Maths
% Need to look at the papers before DT's


% Do we need to talk about the JAX things in the "understanding" component? Probably only briefly as its not **PHYSICS** sooooo
% Will need to talk about auto-diff. Link to JAX papers.
% Intro to gradient descent
	% Loss functions, what are they


\subsection{Automatic-Differentiation, XLA, and Google's JAX}

Automatic-differentiation is the process of computing the differential of \enquote{regular} code, ie. code that is substantially similar to code that one would normally write, with respect to one or more of its arguments. Google's JAX library \cite{jax2018github} contains one implementation of this in Python which works by passing \enquote{tracers} into the Python code which record the actions done to them such that a differential can be calculated.

JAX also incorporates the XLA (Accelerated Linear Algebra) library \cite{openxla-xla} where it uses similar tracing methods to translate a restricted, but large, subset of Python code into a series of low-level eifficent operations which can be executed at speed on the GPU or CPU.

These two technologies together show great promise in the fields of machine learning and simulation techniques as they allow for code that would previously have to be written in low-level languages such as C or C++ to be expressed in Python with significantly reduced overhead at runtime, and easier use by researchers.

\section{Method}

\subsection{Improvements to the \SI{} and their Physical Applications}

% What is the method for the JAX components
% I imagine learning from the DLA report the answer is... don't
% So what do we need to mention here? Probably the physical systems and methods used to measure performance, system size, etc.

\newcommand{\orgimpl}{\textit{original implementation}}
\newcommand{\updimpl}{\textit{updated implementation}}

% TODO: Mention the transition from arbitrary precision to 64bit in the codebase prior.
Initially the existing \texttt{slimpletic} codebase \cite{originalCode}, the \orgimpl{}, was rewritten using the JAX framework, the \updimpl{}. To measure the effectiveness of this rewrite for modelling physical systems TODO scenarios were picked out for modelling.

We create NNN comparative setups to show the applications of our \updimpl{} when compared to the \orgimpl{}. More at once, for longer. Easier exploration of configuration space through additional\_data

First to ensure we have preserved the required error characteristics as discussed in \sref{sec:intro-si} we compare the error of Runge-Kutta, the \orgimpl{}, and the \updimpl{}.

Next we measure the execution speed of the \updimpl{} when compared to the \orgimpl{} on a number of different candidate systems across different numbers of timesteps.

Finally we show the physical applications of this improvement by modelling a MD simulation as discussed in TODO across a different number of bodies and timesteps.

% TODO: Can we use the sytem from Comp A?

\subsection{Applications to Loss Functions}

Taking this \updimpl{} we then explore different applications of it as a loss function, comparing the resulting mathematical properties to those discussed in \sref{sec:intro-lf} and converge properties.

To perform this comparison we chose different embeddings, representations of the non-conservative Lagrangian $\Lambda$.

We then applied gradient-descent\cite{gradientDescent} to these different loss functions, measuring the rate and percentage of convergence across the domain of the embedding and explored different methods for increasing these quantities as well as exploring regions of convergence under different choices of embeddings to test their properties.


% Then discuss auto-diff and the physical systems we chose to fit toward and why.
% How do the systems we fit towards relate to the through-line system?
% Different loss functions we chose to explore and why
% How we compare different loss functions
% What we did, how we built loss functios

\todo{Make the choice of embedding a thing we talk about in results}
\todo{Again wouldn't it be call to talk about symmetries here}

\subsection{Loss Function in the Training of Neural Networks}

Finally we attempt to apply our chosen loss function and Lagrangian embedding to different architectures of neural network determining their effectiveness at determining

\section{Results and Specific Discussion}
% This is the shit we got out of the method
% Essentially 
% These are the error graphs similar to DTs paper (cite DT here)
% We got X% speed up
% This means we can model Y% larger systems for Z% longer time frames
% Results of auto diff results with different loss functions
% If we can make those nice convergence/non-convergence Mandlebrot plots try that!

%\lipsum

\section{General Discussion}
% What do the pretty graphs actually mean in a broad sense
% Pull together with the background
% PHYSICS!!!

% Is this where we will mention the "future work" of NNs?

%\lipsum

\section{Conclusion}

\ack

\begin{enumerate}
	\item Tsang, resources
	\item Do we need to reference the Royal Society?
	\item Mathilda, Dunkun
\end{enumerate}

\section*{References}
\bibliographystyle{iopart-num}
\bibliography{references,static}

\end{document}
