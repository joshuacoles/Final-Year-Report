\documentclass[10pt]{iopart}
\include{setup}

\def\SI{Slimpletic Integrator }
\def\autodiff{automatic-differentiation }

\begin{document}
\ioptwocol[{
\title{Iterations on the slimpletic integrator and its applications to physically informed loss functions}
\author{J D Coles$^1$}
\address{$^1$ Department of Physics, University of Bath, Claverton Down, Bath BA2 7AY, UK}
\begin{abstract}
\lipsum[1]
\end{abstract}
}]

\section{Introduction}
% Heres all the knowledge you need to understand shit

% Following DT's SI paper, we start with top line introductions of the main areas of our paper

Neural networks (NNs) are becoming an increasingly important part of the scientific discipline as they show wider application to data intensive problems and make progress where computational methods have proved otherwise intractable. % TODO: Expand

The loss function, or equivalently reward functions, is used to train a model by optimising its parameters with respect to the aggregate loss over a large collection of known inputs and outputs. They are the component of model creation which ground the model in a given problem. Their construction varies but they generally exhibit some characteristics as discussed later in this paper. % TODO: This feels bitty and a bit undergrad

The \SI (SI)\cite{tsangSLIMPLECTICINTEGRATORSVARIATIONAL2015} is a non-conservative extension of the the Sympletic Integrator, a conservative variational integrator, which enables numerical integration of non-conservative systems with well defined bounds on the energy in the error of the system. These are based on the non-conservative action approach developed by Galley and Galley et al \todo{cite and maybe namecite}.

Automatic-differentiation \todo{do we capitalise both words?} techniques are a collection of computational methods for the determination of the differential of large classes of \enquote{regular} scientific code in an efficient and accurate manner.

% Link these together into a cohesive summary.

In this paper we focus on taking the existing mathematical framework of the \SI and adapting it to use more advanced computational methods and making it amenable to \autodiff to enable a wide class of new applications. Key among these is its use as the baseline of a number of physics based \todo{keep consistent terminology here. I quite like physics based} loss functions which we will compare for empirical and theoretical suitability, with the goal that these could be used in the creation of machine learning models which encode physical knowledge and insight.


% TODO: I note that DT doesn't actually talk about his paper in the introduction? Actually he does at the end

How do we talk about the SI before we talk about the maths?
\begin{enumerate}
	\item What is a slimpletic integrator
	\item What are they useful for
	\item What is the need to auto-diff and a higher speed one
	\item What \PHYSICS does this let us do?
	\item How do we get to autodiff
	\item Introduce the physical system which will be the through line of our model.
\end{enumerate}

% TODO Where do we put applications of autodiff?
\subsection{The \SI}

Symplectic integrators are a class of numerical integrators for Hamiltonian systems which preserve the canonical symplectic 2-form of the system.% Cite: https://en.wikipedia.org/wiki/Symplectic_integrator and DT's unpublished paper.
This makes them widely applicable to physical fields such as orbital dynamics and  molecular dynamics, among others \todo{Cite papers, some from DT2, some of my own}. This is due to their property of preserving, or near preserving, the constants of motion of a system over a large number of integration steps.

\subsection{Non-Conservative Actions}
\subsection{Loss functions}

% Slimpletic Integrator Maths
% Need to look at the papers before DT's

% Do we need to talk about the JAX things in the "understanding" component? Probably only briefly as its not **PHYSICS** sooooo
% Will need to talk about auto-diff. Link to JAX papers.
% Intro to gradient descent
	% Loss functions, what are they

\section{Method}
% What is the method for the JAX components
% I imagine learning from the DLA report the answer is... don't
% So what do we need to mention here? Probably the physical systems and methods used to measure performance, system size, etc.
% Then discuss auto-diff and the physical systems we chose to fit toward and why.
% How do the systems we fit towards relate to the through-line system?
% Different loss functions we chose to explore and why
% How we compare different loss functions

% What we did, how we built loss functios

\section{Results and Specific Discussion}
% This is the shit we got out of the method
% Essentially 
% These are the error graphs similar to DTs paper (cite DT here)
% We got X% speed up
% This means we can model Y% larger systems for Z% longer time frames
% Results of auto diff results with different loss functions
% If we can make those nice convergence/non-convergence Mandlebrot plots try that!

%\lipsum

\section{General Discussion}
% What do the pretty graphs actually mean in a broad sense
% Pull together with the background
% PHYSICS!!!

% Is this where we will mention the "future work" of NNs?

%\lipsum

\section{Conclusion}

\ack

\begin{enumerate}
	\item Tsang, resources
	\item Do we need to reference the Royal Society?
	\item Mathilda, Dunkun
\end{enumerate}

\section*{References}
\bibliographystyle{iopart-num}
\bibliography{references}

\end{document}
