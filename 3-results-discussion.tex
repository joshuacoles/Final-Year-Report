\section{Results and Specific Discussion}

\input{results-si.tex}

\input{results-lf.tex}

\input{results-pinn.tex}

\section{Discussion}

\subsection{\SI{}}

We have presented a number of results on the performance characteristics of the \updimpl{} of the \SI{} method when applied to physical problems which warrant additional discussion to place them in context.

Starting with the effect of changing the method order $r$, as presented in \fref{fig:dho-r-runtime}, roughly constant behaviour is observed and expected to continue for higher values of $r$ as this reflects very little change in the size of the underlying work (due to data-size and calculation cost) involved in the integration process. This coupled with the ability to split long integration runs (high $\Niter$ values) into multiple successive runs means that with careful management of errors, we should be able limit to the degradation to linear performance observed in \fref{fig:dho-n-runtime} for $\Niter > 10^7$ where currently internal arrays become greater than 1Gb in size possibly hitting JAX de-optimisation. \todo{Mum suggests rewriting this sentence}

It is important to note however that in the high order domain $r \gtrsim 40$ the computation of the derivative matrix, given in Equation \eqref{eq:dij}, begins to fail as we encounter issues with the \texttt{float64} floating point precision used in JAX calculations. Unfortunately, this is unavoidable as it is the maximum precision offered by JAX. Nonetheless, this limitation can be mitigated, whilst retaining the desired error characteristics, by employing alternative strategies, such as reducing the time step or using a higher-order method with a lower value.

Overall however this approach is made more fruitful by noting that we can avoid the fixed cost entirely if we reuse the same of form non-conservative Lagrangian. This allows us to vary not only initial conditions, for example exploring the same system in different circumstances or restarting after multiple runs, but also sample across a range of physical parameters (such as those parameterising the DHO 3 embedding as defined in Equation \eqref{eq:dho-sys-embedding}). In particular this scaling without fixed costs applies effectively to systems composed of many repeated sub-systems such as field theory or molecular dynamics simulations \cite{tuckermanUnderstandingModernMolecular2000b}.

Further, as shown in \fref{fig:dho-n-runtime}, we observe a transient increase in runtime over the range $r \in [7, 9]$. This phenomenon is repeatable and its origin is uncertain. However, informed by the size of the quadrature array being $r + 2$, we suspect that this behaviour is due to optimisation cliffs in the JAX internal code as we transition between two internal implementations optimised for small vs large array sizes. Finally, it is important to consider that for less trivial systems, where evaluating the Lagrangian may have its own performance impacts, this may require further analysis and optimisation. \todo{check this reads} This class of complexity management however is a well studied and understood trade off.

Looking forwards, we noted that the use of fixed time steps in the method limits our ability to maintain energy and momentum fractional error bounds. Moving to an adaptive time step approach would be a promising avenue for improvement in future work.

\subsection{PINN, Current and Future}

The trained neural network represents a promising first step in predicting non-conservative Lagrangians from observational data successfully capturing the general characteristics of the data.
During our experiments, we frequently observed predicted embeddings with values of roughly correct proportion, but off by a constant factor. This suggests that the 4 parameter embedding or other normalisation method may have been useful as the system was unable to move past this local minima in phase space, representing a sufficiently similar physical system.
This poses questions on the handling of degeneracy in our Lagrangian space where multiple Lagrangians can result in the same physical system, especially within the image of any chosen embedding function, as this degeneracy will present issues when optimising within the space.

Furthermore, we noted that the model was substantially more accurate with comparatively high masses. We theorise that this may be due to difficulties in fitting larger values of $\vbq$, possibly indicating the need for a larger and more varied dataset. It is important to acknowledge that this was a simplified test network, and we anticipate that future work will achieve improvements by utilising a larger parameter count and datasets.

To enhance the model's performance, incorporating physically known a priori information into the loss function may be beneficial. For example, a stricter restriction on non-negativity for the DHO 3 parameters could be implemented by first taking the absolute value of the outputs as inputs to the Slimpletic Integrator. However, this approach presents a trade-off, as we specialise towards particular physical systems where we have more physical knowledge, but create models that are less widely applicable. Identifying physical laws that can be enforced in the Lagrangian would be a fruitful avenue for future research.

In the same vein, a suitably trained neural network could be constructed using this method to identify symmetries in observational data or recognise patterns it has previously been exposed to in new observational data. This suggests potential applications in domains such as astrophysics, where there is a large availability of data to train a model, but also difficulties in obtaining a complete dataset for any one experiment.

\section{Conclusion}

In this paper we have put forward a more optimal implementation of the Slimpletic Integrator, which is capable of scaling to large scale physical simulations while preserving bounded fractional error in the energy. We have shown this implementation has applications in the construction of a loss function for physics informed neural networks, and presented a simple model which shows that this approach has promise for identifying non-conservative Lagrangians from observational data. More work is required to explore better encodings of physical invariants in the loss function and model structure.
