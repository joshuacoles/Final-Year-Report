\section{Results and Specific Discussion}

\input{results-si.tex}

\input{results-lf.tex}

\input{results-pinn.tex}

\section{Discussion}

\subsection{\SI{}}

We have presented a number of results on the performance characteristics of the \updimpl{} of the \SI{} method applied to physical problems. These warrant additional discussion to place them in context and examine their implications to experimental work.

Starting with the effect of changing the method order $r$, as presented in \fref{fig:dho-r-runtime}.
Roughly constant-time behaviour is observed and is expected to continue for higher values of $r$, as this reflects very little change in the size of the underlying calculation (due to data-size and calculation cost) involved in the integration process.
This, coupled with the ability to split long integration runs (high $\Niter$ values) into multiple successive runs, means, that with careful management of errors (especially with regards to round-off build up due to fixed precision floats), we should be able to limit the degradation to linear performance observed in \fref{fig:dho-n-runtime} for $\Niter > 10^7$.
From this $\Niter$ value the method's internal arrays become greater than 1Gb in size possibly hitting JAX de-optimisation.

It is important to note however, that in the high order domain $r \gtrsim 40$ the computation of the derivative matrix, given in Equation \eqref{eq:dij}, begins to fail as we encounter issues with the \texttt{float64} floating point precision used in JAX calculations. Unfortunately, this is unavoidable as it is the maximum precision offered by JAX. Nonetheless, this limitation can be mitigated, whilst retaining the desired error characteristics by employing alternative strategies, such as reducing the time step or using a higher-order method with a lower value.

Overall, however this approach is made more fruitful by noting that we can avoid the fixed cost entirely, if we reuse the same of form non-conservative Lagrangian.
This allows us to vary not only initial conditions, for example exploring the same system in different circumstances or restarting after multiple runs, but also sample across a range of physical parameters -- for example those parameterising the DHO 3 embedding as defined in Equation \eqref{eq:dho-sys-embedding}.
In particular this scaling without fixed costs applies effectively to systems composed of many repeated sub-systems, such as field theory or molecular dynamics simulations \cite{tuckermanUnderstandingModernMolecular2000b}.

As shown in \fref{fig:dho-n-runtime}, we observe a transient increase in runtime over the range $r \in [7, 9]$. This phenomenon is repeatable and its origin is uncertain. However, informed by the size of the quadrature array being $r + 2$, we suspect that this behaviour is due to optimisation cliffs in the JAX internal code, as we transition between two internal implementations optimised for small vs large array sizes. Finally, it is important to consider that for less trivial systems, where evaluating the Lagrangian may bring its own performance considerations, the method may require further analysis and optimisation.

Looking forwards, we noted that the use of fixed time steps in the method limits our ability to maintain energy and momentum fractional error bounds. Moving to an adaptive time step approach would be a promising avenue for improvement in future work.

\subsection{PINN, Current and Future}

The trained neural network represents a promising first step in predicting non-conservative Lagrangians from observational data, successfully capturing the general characteristics of the data.
During our experiments, we frequently observed predicted embeddings with values of roughly correct proportion, but off by a constant factor.
This suggests that the 4 parameter embedding, or other normalisation methods, may have been useful as the model was unable to move past this local minima in phase space, representing a sufficiently similar physical system.
This poses questions on the handling of degeneracy in our Lagrangian space, where multiple Lagrangians can result in the same physical system, especially within the image of any chosen embedding function, as this degeneracy will present issues when optimising within the space.

Furthermore, we noted that the model was substantially more accurate with comparatively high masses. We theorise that this may be due to difficulties in fitting larger values of $\vbq$, possibly indicating the need for a larger and more varied dataset. It is important to acknowledge that this was a simplified test network, and we anticipate that future work will achieve improvements by utilising a larger parameter count and datasets.

To enhance the model's performance, incorporating physically known a priori information into the loss function may be beneficial. For example, a stricter restriction on non-negativity for the DHO 3 parameters could be implemented by first taking the absolute value of the outputs as inputs to the Slimplectic Integrator. This would have the effect of cleanly solving the problem of non-physical embedding values, at the cost of introducing additional degeneracy into our space possible confusing the model.

It should be noted however, that this and similar approaches, present a trade-off between specialising our embedding and loss functions towards a particular class of physical systems where we have more physical knowledge, and on the other hand, creating models which are more widely applicable, where we are unable to draw such physically derived conclusions (for example when using Taylor series embedding functions).
Identifying physical laws that can be enforced in the Lagrangian would be a fruitful avenue for future research.

Following this theme, a suitably trained neural network could be constructed using this method to identify symmetries in observational data or recognise patterns it has previously been exposed to in new observational data. This suggests potential applications in domains such as astrophysics, where there is a large availability of data to train a model, but also difficulties in obtaining a complete dataset for any one experiment.

\section{Conclusion}

In this paper we have put forward a more optimal implementation of the Slimplectic Integrator, which is capable of scaling to large-scale physical simulations while preserving bounded fractional error in the energy. We have shown this implementation has applications in the construction of a loss function for physics informed neural networks, and presented a simple model which shows that this approach has promise for identifying non-conservative Lagrangians from observational data. More work is required to explore better encodings of physical invariants in the loss function and model structure.
